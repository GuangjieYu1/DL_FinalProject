{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNames = [\"convnext-unetplusplus-espcn-modified\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import PIL.Image as pil\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict\n",
    "from Models.EncoderModel import EncoderModelResNet, EncoderModelConvNeXt\n",
    "from Models.DecoderModel import DepthDecoderModelUNET, DepthDecoderModelUNETPlusPlus, PoseDecoderModel\n",
    "from Models.CameraNet import CameraNet\n",
    "from utils import dispToDepth, transformParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_models = []\n",
    "decoder_models = []\n",
    "for modelName in modelNames:\n",
    "    path = os.path.join(\"models/{}\".format(modelName), \"weights_19\")\n",
    "    if \"resnet\" in modelName:\n",
    "        enc = EncoderModelResNet(50)\n",
    "    elif \"convnext\" in modelName:\n",
    "        enc = EncoderModelConvNeXt()\n",
    "    else:\n",
    "        raise Exception(\"Encoder not found.\")\n",
    "    encoderDict = torch.load(os.path.join(path, \"encoder.pth\"), map_location=device)\n",
    "    try:\n",
    "        height = encoderDict.pop(\"height\")\n",
    "        width = encoderDict.pop(\"width\")\n",
    "        use_stereo = encoderDict.pop(\"use_stereo\")\n",
    "    except:\n",
    "        pass\n",
    "    enc.load_state_dict(encoderDict)\n",
    "    enc.to(device)\n",
    "    z = enc.eval()\n",
    "    espcn = False\n",
    "    if \"espcn\" in modelName:\n",
    "        espcn = True\n",
    "    if \"unetplusplus\" in modelName:\n",
    "        depthDecoder = DepthDecoderModelUNETPlusPlus(enc.numChannels, espcn)\n",
    "    elif \"unet\" in modelName:\n",
    "        depthDecoder = DepthDecoderModelUNET(enc.numChannels, espcn)\n",
    "    else:\n",
    "        raise Exception(\"Decoder not found.\")\n",
    "    try:\n",
    "        depthDecoder.load_state_dict(torch.load(os.path.join(path, \"decoder.pth\"), map_location=device))\n",
    "    except:\n",
    "        odict = torch.load(os.path.join(path, \"depth.pth\"), map_location=device)\n",
    "        odict_compat = OrderedDict([(key.replace(\"conv.conv\", \"conv\"), value) for key, value in odict.items()])\n",
    "        del odict\n",
    "        depthDecoder.load_state_dict(odict_compat)\n",
    "    depthDecoder.to(device)\n",
    "    z = depthDecoder.eval()\n",
    "    \"\"\"if \"camnet\" in modelName:\n",
    "        poseDecoder = CameraNet(enc.numChannels[-1], 192//8, 640//8)\n",
    "    else:\n",
    "        poseDecoder = PoseDecoderModel(enc.numChannels, 2, 1)\n",
    "    poseDecoder.load_state_dict(torch.load(os.path.join(path, \"pose.pth\"), map_location=device))\n",
    "    poseDecoder.to(device)\n",
    "    z = poseDecoder.eval()\"\"\"\n",
    "    encoder_models.append(enc)\n",
    "    decoder_models.append(depthDecoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrame(video_in, frame_idx):\n",
    "    video_in.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "    countWait = 0\n",
    "    ok = True\n",
    "    frame = None\n",
    "    while ok:\n",
    "        ok, frame = video_in.read()\n",
    "        if not ok:\n",
    "            print(\"Waiting ... \")\n",
    "            sleep(1)\n",
    "            countWait += 1\n",
    "            if countWait < 5:\n",
    "                ok = True\n",
    "            else:\n",
    "                ok = False\n",
    "        else:\n",
    "            ok = False\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames : 601\n",
      "Frames completed: 50\n",
      "Frames completed: 100\n",
      "Frames completed: 150\n",
      "Frames completed: 200\n",
      "Frames completed: 250\n",
      "Frames completed: 300\n",
      "Frames completed: 350\n",
      "Frames completed: 400\n",
      "Frames completed: 450\n",
      "Frames completed: 500\n",
      "Frames completed: 550\n"
     ]
    }
   ],
   "source": [
    "video_in = \"predictions/testVideo.mp4\"\n",
    "video_in = cv2.VideoCapture(video_in)\n",
    "fps = video_in.get(cv2.CAP_PROP_FPS)\n",
    "width = int(video_in.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video_in.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "length = int(video_in.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(\"Total frames : {}\".format(length))\n",
    "video_outs = []\n",
    "for modelName in modelNames:\n",
    "    video_outs.append(cv2.VideoWriter(\"predictions/testVideo-{}.mp4\".format(modelName), cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height)))    \n",
    "checks = 0\n",
    "fc = 0\n",
    "while video_in.isOpened():\n",
    "    with torch.no_grad():\n",
    "        status, frame_T = video_in.read()\n",
    "        if not status:\n",
    "            checks += 1\n",
    "            if checks == 10:\n",
    "                video_in.release()\n",
    "            continue\n",
    "        else:\n",
    "            checks = 0\n",
    "        frame_T = pil.fromarray(cv2.cvtColor(frame_T, cv2.COLOR_BGR2RGB))\n",
    "        original_frame = frame_T.copy()\n",
    "        frame_T = frame_T.resize((640, 192), pil.LANCZOS)\n",
    "        frame_T = transforms.ToTensor()(frame_T).unsqueeze(0).to(device)\n",
    "        for enc, depthDecoder, video_out in zip(encoder_models, decoder_models, video_outs):\n",
    "            main_features = enc(frame_T)\n",
    "            outputs = depthDecoder(main_features)\n",
    "            disp = outputs[(\"disp\", 0)]\n",
    "            disp_resized_np = torch.nn.functional.interpolate(disp, (height, width), mode=\"bilinear\", align_corners=True).squeeze().cpu().numpy()\n",
    "            normalizer = mpl.colors.Normalize(vmin=disp_resized_np.min(), vmax=np.percentile(disp_resized_np, 95))\n",
    "            mapper = cm.ScalarMappable(norm=normalizer, cmap='magma')\n",
    "            im = (mapper.to_rgba(disp_resized_np)[:, :, :3] * 255).astype(np.uint8)\n",
    "            video_out.write(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "        fc += 1\n",
    "        if fc % 50 == 0:\n",
    "            print(\"Frames completed: {}\".format(fc))\n",
    "for video_out in video_outs:\n",
    "    video_out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
